{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "When we call `fit()` on an estimator, it learns the parameters of the algorithm that make it fit the data best. However, some parameters are not directly learned within an estimator. These parameters are often referred to as hyperparameters, and include thing like:\n",
    "\n",
    "- depth of a decision tree\n",
    "- alpha for regularization\n",
    "- kernel for support vector machines\n",
    "- number of clusters for centroidal clustering\n",
    "\n",
    "In this notebook we'll investigate some techniques for exploring and optimizing hyperparameters to improve the performance of our machine learning models.\n",
    "\n",
    "\n",
    "## Load and Prepare the Data\n",
    "\n",
    "  \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "First, go to your `select/data` folder, and unzip the zipped files!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "!    \n",
    "  \n",
    "\n",
    "## Learning the Sensible Defaults\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "How do we prevent overfit in our machine learning models? One strategy is to use regularization to affect *smoothing* in the data. \n",
    "\n",
    "Regularization is designed to penalize model complexity, therefore the higher the alpha, the less complex the model, decreasing the error due to variance (overfit). Alphas that are too high on the other hand increase the error due to bias (underfit). It is important, therefore to choose an optimal alpha such that the error is minimized in both directions.\n",
    "\n",
    "The scikit-learn library offers a few popular techniques for regularization, including `LASSO`, `Ridge`, and `ElasticNet`:\n",
    "\n",
    "- [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) (L1 Regularization) is a linear model trained with L1 prior as the regularizer. Lasso forces weak features to have zeroes as coefficients, effectively dropping the least predictive features. Technically the Lasso model is optimizing the same objective function as the Elastic Net with `l1_ratio=1.0` (no L2 penalty).\n",
    "- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) Regression (L2 Regularization) solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Ridge assigns every feature a weight, but spreads the coefficient values out more equally, shrinking but still maintaining less predictive features.\n",
    "- [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) performs a linear regression with combined L1 and L2 priors as a regularizer.\n",
    "\n",
    "\n",
    "Each regularizer has an `alpha` hyperparameter that helps to determine how much smoothing to do.\n",
    "\n",
    "What are the default alpha values for each of the regularizers? What happens when you change them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "data = pd.read_csv(\"data/occupancy/datatraining.txt\")\n",
    "X = data[[\"Temperature\", \"Humidity\", \"Light\", \"CO2\", \"HumidityRatio\"]].values\n",
    "y = data[\"Occupancy\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso() # what does alpha default to? What happens when you change it?\n",
    "model.fit(X, y)\n",
    "print(list(zip(X, model.coef_.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge() # what does alpha default to? What happens when you change it?\n",
    "model.fit(X, y)\n",
    "print(list(zip(X, model.coef_.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElasticNet() # what does alpha default to? What happens when you change it?\n",
    "model.fit(X, y)\n",
    "print(list(zip(X, model.coef_.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch\n",
    "\n",
    "Gridsearch is a method for finding the best combination of hyperparameters via an exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "When you do a gridsearch, scikit-learn creates a new model for each possible combination of hyperparameters. Each of these combinations is a point on the search grid. Gridsearch trains each of these models and evaluates them using cross-validation, and then provides the results for the one that performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = pd.read_csv(\"data/energy/energy.csv\")\n",
    "X = data[['relative compactness', 'surface area', 'wall area', 'roof area',\n",
    "       'overall height', 'orientation', 'glazing area',\n",
    "       'glazing area distribution']].values\n",
    "y = data['heating load'].values\n",
    "\n",
    "ridge = Ridge(random_state=0)\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "tuned_params = [{'alpha': alphas}]\n",
    "n_folds = 5\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    ridge, tuned_params, cv=n_folds\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also:\n",
    "\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "- [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)\n",
    "\n",
    "\n",
    "## Visual Gridsearch\n",
    "\n",
    "### AlphaSelection\n",
    "The `AlphaSelection` Visualizer demonstrates how different values of alpha influence model selection during the regularization of linear models. Generally speaking, alpha increases the affect of regularization, e.g. if alpha is zero there is no regularization and the higher the alpha, the more the regularization parameter influences the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "\n",
    "# Load the regression dataset\n",
    "data = pd.read_excel(\"data/concrete.xls\", header=0)\n",
    "\n",
    "X = data[['Cement (component 1)(kg in a m^3 mixture)',\n",
    "       'Blast Furnace Slag (component 2)(kg in a m^3 mixture)',\n",
    "       'Fly Ash (component 3)(kg in a m^3 mixture)',\n",
    "       'Water  (component 4)(kg in a m^3 mixture)',\n",
    "       'Superplasticizer (component 5)(kg in a m^3 mixture)',\n",
    "       'Coarse Aggregate  (component 6)(kg in a m^3 mixture)',\n",
    "       'Fine Aggregate (component 7)(kg in a m^3 mixture)', 'Age (day)']].values\n",
    "y = data['Concrete compressive strength(MPa, megapascals) '].values\n",
    "\n",
    "# Create some lists of alphas to cross-validate against\n",
    "small_range = np.logspace(-10, 1, 400)\n",
    "medium_range = np.logspace(-10, 2, 400)\n",
    "large_range = np.logspace(-10, 4, 400)\n",
    "\n",
    "smoothers = {\n",
    "    \"Lasso\": LassoCV(cv=5, alphas=small_range),\n",
    "    \"Ridge\": RidgeCV(store_cv_values=True, alphas=large_range),\n",
    "    \"ElasticNet\": ElasticNetCV(cv=5, alphas=medium_range)\n",
    "}\n",
    "\n",
    "for _, smoother in smoothers.items():\n",
    "    _, ax = plt.subplots() # Create a new figure\n",
    "    visualizer = AlphaSelection(smoother, size=(1080, 720))\n",
    "    visualizer.fit(X, y)\n",
    "    visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AlphaSelection` class expects an estimator whose name ends with \"CV\". If you wish to use some other estimator, please see the `ManualAlphaSelection` Visualizer for manually iterating through all alphas and selecting the best one.\n",
    "\n",
    "### Other Hyperparameter Tuning Resources\n",
    "\n",
    "For more about hyperparameter tuning with Yellowbrick, check out:\n",
    "\n",
    "- [Validation Curve Visualizer](https://www.scikit-yb.org/en/develop/api/model_selection/validation_curve.html)\n",
    "- [Silhouette Visualizer](https://www.scikit-yb.org/en/develop/api/cluster/silhouette.html)\n",
    "- [Elbow Curve Visualizer](https://www.scikit-yb.org/en/develop/api/cluster/elbow.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
